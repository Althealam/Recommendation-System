{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练数据\n",
    "# 1. 用户侧：用户画像属性（用户性别、年龄、所在省市、使用设备及系统）\n",
    "# 2. 新闻侧：新闻的创建时间、题目、所属一级、二级类别，题片个数以及关键词\n",
    "def proccess(file):\n",
    "    if file==\"user_info_data_5w.csv\":\n",
    "        data = pd.read_csv(file_path + file, sep=\"\\t\",index_col=0)\n",
    "        data[\"age\"] = data[\"age\"].map(lambda x: get_pro_age(x))\n",
    "        data[\"gender\"] = data[\"gender\"].map(lambda x: get_pro_age(x))\n",
    "\n",
    "        data[\"province\"]=data[\"province\"].fillna(method='ffill')\n",
    "        data[\"city\"]=data[\"city\"].fillna(method='ffill')\n",
    "\n",
    "        data[\"device\"] = data[\"device\"].fillna(method='ffill')\n",
    "        data[\"os\"] = data[\"os\"].fillna(method='ffill')\n",
    "        return data\n",
    "\n",
    "    elif file==\"doc_info.txt\":\n",
    "        data = pd.read_csv(file_path + file, sep=\"\\t\")\n",
    "        data.columns = [\"article_id\", \"title\", \"ctime\", \"img_num\",\"cate\",\"sub_cate\", \"key_words\"]\n",
    "        select_column = [\"article_id\", \"title_len\", \"ctime\", \"img_num\",\"cate\",\"sub_cate\", \"key_words\"]\n",
    "\n",
    "        # 去除时间为nan的新闻以及除脏数据\n",
    "        data= data[(data[\"ctime\"].notna()) & (data[\"ctime\"] != 'Android')]\n",
    "        data['ctime'] = data['ctime'].astype('str')\n",
    "        data['ctime'] = data['ctime'].apply(lambda x: int(x[:10]))\n",
    "        data['ctime'] = pd.to_datetime(data['ctime'], unit='s', errors='coerce')\n",
    "\n",
    "\n",
    "        # 这里存在nan字符串和异常数据\n",
    "        data[\"sub_cate\"] = data[\"sub_cate\"].astype(str)\n",
    "        data[\"sub_cate\"] = data[\"sub_cate\"].apply(lambda x: pro_sub_cate(x))\n",
    "        data[\"img_num\"] = data[\"img_num\"].astype(str)\n",
    "        data[\"img_num\"] = data[\"img_num\"].apply(photoNums)\n",
    "        data[\"title_len\"] = data[\"title\"].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "        data[\"cate\"] = data[\"cate\"].fillna('其他')\n",
    "\n",
    "     return data[select_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4499e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练样本\n",
    "# 根据用户的交互日志中前六天的数据作为训练集，第七天的数据作为测试集，来构造模型的训练测试样本\n",
    "def dealsample(file, doc_data, user_data, s_data_str = \"2021-06-24 00:00:00\", e_data_str=\"2021-06-30 23:59:59\", neg_num=5):\n",
    "    # 先处理时间问题\n",
    "    data = pd.read_csv(file_path + file, sep=\"\\t\",index_col=0)\n",
    "    data['expo_time'] = data['expo_time'].astype('str')\n",
    "    data['expo_time'] = data['expo_time'].apply(lambda x: int(x[:10]))\n",
    "    data['expo_time'] = pd.to_datetime(data['expo_time'], unit='s', errors='coerce')\n",
    "\n",
    "    s_date = datetime.datetime.strptime(s_data_str,\"%Y-%m-%d %H:%M:%S\")\n",
    "    e_date = datetime.datetime.strptime(e_data_str,\"%Y-%m-%d %H:%M:%S\") + datetime.timedelta(days=-1)\n",
    "    t_date = datetime.datetime.strptime(e_data_str,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # 选取训练和测试所需的数据\n",
    "    all_data_tmp = data[(data[\"expo_time\"]>=s_date) & (data[\"expo_time\"]<=t_date)]\n",
    "\n",
    "    # 处理训练数据集  防止穿越样本\n",
    "    # 1. merge 新闻信息，得到曝光时间和新闻创建时间； inner join 去除doc_data之外的新闻\n",
    "    all_data_tmp = all_data_tmp.join(doc_data.set_index(\"article_id\"),on=\"article_id\",how='inner')\n",
    "\n",
    "    # 发现还存在 ctime大于expo_time的交互存在  去除这部分错误数据\n",
    "    all_data_tmp = all_data_tmp[(all_data_tmp[\"ctime\"]<=all_data_tmp[\"expo_time\"])]\n",
    "\n",
    "    # 2. 去除与新闻的创建时间在测试数据时间内的交互  ()\n",
    "    train_data = all_data_tmp[(all_data_tmp[\"expo_time\"]>=s_date) & (all_data_tmp[\"expo_time\"]<=e_date)]\n",
    "    train_data = train_data[(train_data[\"ctime\"]<=e_date)]\n",
    "\n",
    "    print(\"有效的样本数：\",train_data[\"expo_time\"].count())\n",
    "\n",
    "    # 负采样\n",
    "    if os.path.exists(file_path + \"neg_sample.pkl\") and os.path.getsize(file_path + \"neg_sample.pkl\"):\n",
    "        neg_samples = pd.read_pickle(file_path + \"neg_sample.pkl\")\n",
    "     # train_neg_samples.insert(loc=2, column=\"click\", value=[0] * train_neg_samples[\"user_id\"].count())\n",
    "    else:\n",
    "        # 进行负采样的时候对于样本进行限制，只对一定时间范围之内的样本进行负采样\n",
    "        doc_data_tmp = doc_data[(doc_data[\"ctime\"]>=datetime.datetime.strptime(\"2021-06-01 00:00:00\",\"%Y-%m-%d %H:%M:%S\"))]\n",
    "        neg_samples = negSample_like_word2vec(train_data, doc_data_tmp[[\"article_id\"]].values, user_data[[\"user_id\"]].values, neg_num=neg_num)\n",
    "        neg_samples = pd.DataFrame(neg_samples, columns= [\"user_id\",\"article_id\",\"click\"])\n",
    "        neg_samples.to_pickle(file_path + \"neg_sample.pkl\")\n",
    "\n",
    "    train_pos_samples = train_data[train_data[\"click\"] == 1][[\"user_id\",\"article_id\", \"expo_time\", \"click\"]]    # 取正样本\n",
    "\n",
    "    neg_samples_df = train_data[train_data[\"click\"] == 0][[\"user_id\",\"article_id\", \"click\"]]\n",
    "    train_neg_samples = pd.concat([neg_samples_df.sample(n=train_pos_samples[\"click\"].count()) ,neg_samples],axis=0)  # 取负样本\n",
    "\n",
    "    print(\"训练集正样本数：\",train_pos_samples[\"click\"].count())\n",
    "    print(\"训练集负样本数：\",train_neg_samples[\"click\"].count())\n",
    "\n",
    "    train_data_df = pd.concat([train_neg_samples,train_pos_samples],axis=0)\n",
    "    train_data_df = train_data_df.sample(frac=1)  # shuffle\n",
    "\n",
    "    print(\"训练集总样本数：\",train_data_df[\"click\"].count())\n",
    "\n",
    "    test_data_df =  all_data_tmp[(all_data_tmp[\"expo_time\"]>e_date) & (all_data_tmp[\"expo_time\"]<=t_date)][[\"user_id\",\"article_id\", \"expo_time\", \"click\"]]\n",
    "\n",
    "    print(\"测试集总样本数：\",test_data_df[\"click\"].count())\n",
    "    print(\"测试集总样本数：\",test_data_df[\"click\"].count())\n",
    "\n",
    "    all_data_df =  pd.concat([train_data_df, test_data_df],axis=0)\n",
    "\n",
    "    print(\"总样本数：\",all_data_df[\"click\"].count())\n",
    "\n",
    "    return all_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8951829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负样本采样\n",
    "# 采用基于item的展现次数对全局item进行负采样\n",
    "\n",
    "def negSample_like_word2vec(train_data, all_items, all_users, neg_num=10):\n",
    "    \"\"\"\n",
    "    为所有item计算一个采样概率，根据概率为每个用户采样neg_num个负样本，返回所有负样本对\n",
    "    1. 统计所有item在交互中的出现频次\n",
    "    2. 根据频次进行排序，并计算item采样概率（频次出现越多，采样概率越低，打压热门item）\n",
    "    3. 根据采样概率，利用多线程为每个用户采样 neg_num 个负样本\n",
    "    \"\"\"\n",
    "    pos_samples = train_data[train_data[\"click\"] == 1][[\"user_id\",\"article_id\"]]\n",
    "\n",
    "    pos_samples_dic = {}\n",
    "    for idx,u in enumerate(pos_samples[\"user_id\"].unique().tolist()):\n",
    "        pos_list = list(pos_samples[pos_samples[\"user_id\"] == u][\"article_id\"].unique().tolist())\n",
    "        if len(pos_list) >= 30:  # 30是拍的  需要数据统计的支持确定\n",
    "            pos_samples_dic[u] = pos_list[30:]\n",
    "        else:\n",
    "            pos_samples_dic[u] = pos_list\n",
    "\n",
    "    # 统计出现频次\n",
    "    article_counts = train_data[\"article_id\"].value_counts()\n",
    "    df_article_counts = pd.DataFrame(article_counts)\n",
    "    dic_article_counts = dict(zip(df_article_counts.index.values.tolist(),df_article_counts.article_id.tolist()))\n",
    "\n",
    "    for item in all_items:\n",
    "        if item[0] not in dic_article_counts.keys():\n",
    "            dic_article_counts[item[0]] = 0\n",
    "\n",
    "    # 根据频次排序, 并计算每个item的采样概率\n",
    "    tmp = sorted(list(dic_article_counts.items()), key=lambda x:x[1], reverse=True)  # 降序\n",
    "    n_articles = len(tmp)\n",
    "    article_prob = {}\n",
    "    for idx, item in enumerate(tmp):\n",
    "        article_prob[item[0]] = cal_pos(idx, n_articles)\n",
    "\n",
    "    # 为每个用户进行负采样\n",
    "    article_id_list = [a[0] for a in article_prob.items()]\n",
    "    article_pro_list = [a[1] for a in article_prob.items()]\n",
    "    pos_sample_users = list(pos_samples_dic.keys())\n",
    "\n",
    "    all_users_list = [u[0] for u in all_users]\n",
    "\n",
    "    print(\"start negative sampling !!!!!!\")\n",
    "    pool = multiprocessing.Pool(core_size)\n",
    "    res = pool.map(SampleOneProb((pos_sample_users,article_id_list,article_pro_list,pos_samples_dic,neg_num)), tqdm(all_users_list))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    neg_sample_dic = {}\n",
    "    for idx, u in tqdm(enumerate(all_users_list)):\n",
    "        neg_sample_dic[u] = res[idx]\n",
    "\n",
    "    return [[k,i,0] for k,v in neg_sample_dic.items() for i in v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4833d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
