{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a30f82",
   "metadata": {},
   "source": [
    "### Naive Softmax损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e756d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec, # 中心词的向量表示\n",
    "    outsideWordIdx, # 正确的外部词（上下文词）在向量集中的索引\n",
    "    outsideVectors, # 外部词的向量集合\n",
    "    dataset \n",
    "):\n",
    "    # 通过矩阵乘法计算每个外部词向量与中心词向量的点积，得到一个分数向量\n",
    "    # 这个分数表示每个外部词与中心词的相似度。\n",
    "    scores=np.matmul(outsideVectors, centerWordVec)\n",
    "    # 应用softmax函数，将分数转换为概率分布\n",
    "    # 对于每个外部词，其输出表示在给定中心词的情况下，该外部词被正确预测的概率。\n",
    "    probs=softmax(scores)\n",
    "    \n",
    "    \n",
    "    # 损失是正确外部词的概率的负对数\n",
    "    # softmax回归中的典型损失函数，用于优化模型以正确预测正确的外部词。\n",
    "    loss=-np.log(probs[outsideWordIdx]) # scalar\n",
    "    \n",
    "    # 复制概率分布。\n",
    "    dscores=probs.copy\n",
    "    # 对于正确的外部词，将其对应的概率减去1，得到梯度\n",
    "    dscores[outsideWordIdx]=dscores[outsideWordIdx]-1\n",
    "    # 中心词向量的梯度\n",
    "    gradCenterVec=np.matmul(outsideVectors, dscores) # J关于vc的偏导公式\n",
    "    # 外部词向量的梯度\n",
    "    gradOutsideVecs=np.outer(dscores, centerWordVec) # J关于u的偏导公式\n",
    "    \n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "# loss：损失值\n",
    "# gradCenterVec：中心词向量的梯度\n",
    "# gradOutsideVecs：外部词向量的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34992f2",
   "metadata": {},
   "source": [
    "### 负采样损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70521411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec, # 中心词的向量表示\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    # 在负采样框架下计算词嵌入模型的损失函数和梯度\n",
    "  \n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    gradCenterVec =np.zeros(centerWordVec.shape)  # (embedding_size,1)\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)  # (vocab_size, embedding_size)\n",
    "    loss = 0.0\n",
    "\n",
    "    u_o = outsideVectors[outsideWordIdx]  # size=(embedding_size,1)\n",
    "    z = sigmoid(np.dot(u_o, centerWordVec))  # size=(1, )\n",
    "    loss -= np.log(z) # 损失函数的第一部分\n",
    "    gradCenterVec += u_o * (z - 1)   # J关于vc的偏导数的第一部分\n",
    "    gradOutsideVecs[outsideWordIdx] = centerWordVec * (z - 1)  # J关于u_o的偏导数计算\n",
    "\n",
    "    for i in range(K):\n",
    "        neg_id = indices[1 + i]\n",
    "        u_k = outsideVectors[neg_id]\n",
    "        z = sigmoid(-np.dot(u_k, centerWordVec))\n",
    "        loss -= np.log(z)\n",
    "        gradCenterVec += u_k * (1-z)\n",
    "        gradOutsideVecs[neg_id] += centerWordVec * (1 - z)\n",
    "\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a858a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
