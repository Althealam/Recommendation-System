{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 20:57:48.479364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseFeat:\n",
    "    def __init__(self, name, vocabulary_size, embedding_dim):\n",
    "        self.name = name\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "class DenseFeat:\n",
    "    def __init__(self, name, dimension):\n",
    "        self.name = name\n",
    "        self.dimension = dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设我们有以下用户和物品特征\n",
    "user_feature_columns = [\n",
    "    SparseFeat('user_id', 10000, embedding_dim=64),\n",
    "    DenseFeat('age', 1)\n",
    "]\n",
    "\n",
    "item_feature_columns = [\n",
    "    SparseFeat('item_id', 100000, embedding_dim=64)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建输入层\n",
    "def build_input_layers(feature_columns):\n",
    "    input_layers = {}\n",
    "    for feature in feature_columns:\n",
    "        if isinstance(feature, SparseFeat):  # 假设 SparseFeat 是稀疏特征\n",
    "            input_layers[feature.name] = layers.Input(shape=(1,), name=feature.name, dtype=tf.int32)\n",
    "        elif isinstance(feature, DenseFeat):  # 假设 DenseFeat 是密集特征\n",
    "            input_layers[feature.name] = layers.Input(shape=(feature.dimension,), name=feature.name, dtype=tf.float32)\n",
    "    return input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为给定的特征创建嵌入层\n",
    "# 嵌入层通常用于将稀疏的特征（类别特征）转换为密集的向量表示\n",
    "def build_embedding_layers(feature_columns):\n",
    "    embedding_layers = {}\n",
    "    for feature in feature_columns:\n",
    "        if isinstance(feature, SparseFeat):  # 假设 SparseFeat 是稀疏特征\n",
    "            # 创建一个嵌入层，将词汇表大小的输入映射到指定的嵌入维度\n",
    "            embedding_layers[feature.name] = layers.Embedding(\n",
    "                input_dim=feature.vocabulary_size + 1,  # 词汇表大小 + 1，以处理0索引\n",
    "                output_dim=feature.embedding_dim,\n",
    "                input_length=1,  # 输入长度为1，因为我们处理的是单个特征值\n",
    "                name=f\"{feature.name}_embedding\"\n",
    "            )\n",
    "    return embedding_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_lookup函数\n",
    "# 将输入的稀疏特征通过嵌入层转换为嵌入向量\n",
    "# 通常用于将类别特征转换为密集的低维向量表示\n",
    "\n",
    "def embedding_lookup(feature_names, input_layer_dict, embedding_layer_dict):\n",
    "    \"\"\"\n",
    "    使用嵌入层查找特征的嵌入向量。\n",
    "    \n",
    "    参数:\n",
    "    - feature_names: 要查找的特征名称列表。\n",
    "    - input_layer_dict: 包含输入层的字典。\n",
    "    - embedding_layer_dict: 包含嵌入层的字典。\n",
    "    \n",
    "    返回:\n",
    "    - 嵌入向量的列表。\n",
    "    \"\"\"\n",
    "    embedding_vectors = []\n",
    "    for name in feature_names:\n",
    "        if name in input_layer_dict:\n",
    "            input_layer = input_layer_dict[name]\n",
    "            embedding_layer = embedding_layer_dict[name]\n",
    "            # 使用嵌入层查找输入层的嵌入向量\n",
    "            embedding_vector = embedding_layer(input_layer)\n",
    "            embedding_vectors.append(embedding_vector)\n",
    "        else:\n",
    "            raise ValueError(f\"Feature '{name}' not found in input or embedding layers.\")\n",
    "    return embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoolingLayer通常用于将多个嵌入向量合并为一个\n",
    "from tensorflow.keras.layers import Layer\n",
    "class PoolingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PoolingLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 假设inputs是一个嵌入向量的列表\n",
    "        # 使用平均池化来合并向量\n",
    "        return tf.reduce_mean(inputs, axis=0)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # 返回池化后的输出形状\n",
    "        return input_shape[0]  # 假设所有输入具有相同的形状\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PoolingLayer, self).get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CapsuleLayer是胶囊网络中的核心层\n",
    "class CapsuleLayer(Layer):\n",
    "    def __init__(self, input_units, output_units, max_len, k_max, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.input_units = input_units\n",
    "        self.output_units = output_units\n",
    "        self.max_len = max_len\n",
    "        self.k_max = k_max\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.output_units * self.max_len),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super(CapsuleLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 这里只是一个简化示例，实际的胶囊层会更复杂\n",
    "        inputs = tf.keras.layers.Reshape((self.max_len, self.input_units))(inputs)\n",
    "        outputs = tf.keras.layers.Dot(axes=2)([inputs, self.W])\n",
    "        return tf.keras.layers.Activation('sigmoid')(outputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_units)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CapsuleLayer, self).get_config()\n",
    "        config.update({\n",
    "            'input_units': self.input_units,\n",
    "            'output_units': self.output_units,\n",
    "            'max_len': self.max_len,\n",
    "            'k_max': self.k_max\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN（深度神经网络），由多个全连接层组成\n",
    "class DNN(tf.keras.Model):\n",
    "    def __init__(self, hidden_units, activation, reg, dropout, use_bn, output_activation, seed, **kwargs):\n",
    "        super(DNN, self).__init__(**kwargs)\n",
    "        self.hidden_layers = []\n",
    "        for units in hidden_units:\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(units,\n",
    "                                                            activation=activation,\n",
    "                                                            kernel_regularizer=tf.keras.regularizers.l2(reg),\n",
    "                                                            bias_regularizer=tf.keras.regularizers.l2(reg),\n",
    "                                                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed)))\n",
    "            if use_bn:\n",
    "                self.hidden_layers.append(tf.keras.layers.BatchNormalization())\n",
    "            self.hidden_layers.append(tf.keras.layers.Dropout(dropout))\n",
    "        self.output_layer = tf.keras.layers.Dense(1,\n",
    "                                                  activation=output_activation,\n",
    "                                                  kernel_initializer=tf.keras.initializers.glorot_uniform(seed))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoMask\n",
    "# 用于处理序列掩码的自定义层或者函数\n",
    "import tensorflow as tf\n",
    "\n",
    "class NoMask(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NoMask, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 假设这个层只是简单地返回输入，不进行掩码处理\n",
    "        return inputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(NoMask, self).get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_dnn_input\n",
    "# 用于合并不同类型输入（如嵌入向量和密集特征）\n",
    "def combined_dnn_input(embedding_inputs, dense_inputs):\n",
    "    # 将嵌入向量和密集特征拼接在一起\n",
    "    if embedding_inputs and dense_inputs:\n",
    "        input_combined = tf.keras.layers.Concatenate()(embedding_inputs + dense_inputs)\n",
    "    elif embedding_inputs:\n",
    "        input_combined = tf.keras.layers.Concatenate()(embedding_inputs)\n",
    "    elif dense_inputs:\n",
    "        input_combined = tf.keras.layers.Concatenate()(dense_inputs)\n",
    "    else:\n",
    "        raise ValueError(\"No input provided for DNN.\")\n",
    "    return input_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile_user_otherfeat：\n",
    "# 用于扩展用户特征以匹配胶囊网络输入维度\n",
    "def tile_user_otherfeat(inputs, k_max):\n",
    "    # 将用户其他特征重复k_max次以匹配胶囊网络的输入维度\n",
    "    tiled_inputs = tf.tile(inputs, [1, k_max, 1])\n",
    "    return tiled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelAwareAttention层\n",
    "# 根据输入的keys、query和可选地length来计算注意力权重\n",
    "\n",
    "class LabelAwareAttention(Layer):\n",
    "    def __init__(self, k_max, pow_p=1, **kwargs):\n",
    "        self.k_max = k_max\n",
    "        self.pow_p = pow_p\n",
    "        super(LabelAwareAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_size = input_shape[0][-1]\n",
    "        super(LabelAwareAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        keys = inputs[0]\n",
    "        query = inputs[1]\n",
    "        weight = tf.reduce_sum(keys * query, axis=-1, keepdims=True)\n",
    "        weight = tf.pow(weight, self.pow_p)  # [batch_size, k_max, 1]\n",
    "        if len(inputs) == 3:\n",
    "            k_user = tf.cast(tf.maximum(\n",
    "                1.,\n",
    "                tf.minimum(\n",
    "                    tf.cast(self.k_max, dtype=\"float32\"),\n",
    "                    tf.math.log1p(tf.cast(inputs[2], dtype=\"float32\")) / tf.math.log(2.)\n",
    "                )\n",
    "            ), dtype=\"int64\")\n",
    "            seq_mask = tf.transpose(tf.sequence_mask(k_user, self.k_max), [0, 2, 1])\n",
    "            padding = tf.ones_like(seq_mask, dtype=tf.float32) * (-2 ** 32 + 1)\n",
    "            weight = tf.where(seq_mask, weight, padding)\n",
    "        weight = tf.nn.softmax(weight, name=\"weight\")\n",
    "        output = tf.reduce_sum(keys * weight, axis=1)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.embedding_size)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'k_max': self.k_max, 'pow_p': self.pow_p}\n",
    "        base_config = super(LabelAwareAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmbeddingIndex\n",
    "# 返回一个常数索引\n",
    "class EmbeddingIndex(Layer):\n",
    "    def __init__(self, index, **kwargs):\n",
    "        self.index = index\n",
    "        super(EmbeddingIndex, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(EmbeddingIndex, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        return tf.constant(self.index)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'index': self.index}\n",
    "        base_config = super(EmbeddingIndex, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_item_embedding\n",
    "# 获取物品的嵌入向量\n",
    "def get_item_embedding(pooling_item_embedding_weight, item_input_layer):\n",
    "    # 假设 pooling_item_embedding_weight 是一个嵌入层的输出\n",
    "    # item_input_layer 是对应的输入层\n",
    "    item_embedding = pooling_item_embedding_weight[item_input_layer]\n",
    "    return item_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SampledSoftmaxLayer\n",
    "class SampledSoftmaxLayer(Layer):\n",
    "    def __init__(self, num_sampled, **kwargs):\n",
    "        super(SampledSoftmaxLayer, self).__init__(**kwargs)\n",
    "        self.num_sampled = num_sampled\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 创建层的权重\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        super(SampledSoftmaxLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 这里只是一个简化示例，实际的采样softmax会更复杂\n",
    "        logits = tf.matmul(inputs, self.W) + self.b\n",
    "        sampled_logits = tf.nn.sampled_softmax(logits, self.num_sampled)\n",
    "        return sampled_logits\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SampledSoftmaxLayer, self).get_config()\n",
    "        config.update({'num_sampled': self.num_sampled})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarLenSparseFeat:\n",
    "    def __init__(self, sparsefeat, maxlen, combiner='mean', length_name=None, weight_name=None, weight_norm=True):\n",
    "        self.sparsefeat = sparsefeat\n",
    "        self.maxlen = maxlen\n",
    "        self.combiner = combiner\n",
    "        self.length_name = length_name\n",
    "        self.weight_name = weight_name\n",
    "        self.weight_norm = weight_norm\n",
    "\n",
    "# 示例使用\n",
    "# 假设我们有一个SparseFeat实例，名为sparse_feature\n",
    "sparse_feature = SparseFeat('sparse_feature', 10000, 64)\n",
    "varlen_feature = VarLenSparseFeat(sparse_feature, maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "def MIND(user_feature_columns, item_feature_columns, num_sampled=5, k_max=2,p=1.0, dynamic_k=False, user_dnn_hidden_units=(64,32),\n",
    "         dnn_activation='relu',dnn_use_bn=False, reg_dnn=0, reg_embedding=1e-6, dnn_dropout=0, output_activation='linear', seed=1024):\n",
    "    \"\"\"\n",
    "    user_feature_columns: 用户特征列\n",
    "    item_feature_columns: 物品特征列\n",
    "    num_sampled: 负采样的数量\n",
    "    k_max: 胶囊网络中胶囊的最大数量\n",
    "    p: Label-aware Attention中的指数参数\n",
    "    user_dnn_hidden_units: 用户DNN层的隐藏单元\n",
    "    dnn_use_bn: 是否在DNN层使用批量归一化\n",
    "    reg_dnn: DNN层的正则化系数\n",
    "    reg_embedding: 嵌入层的正则化系数\n",
    "    dnn_dropout: DNN层的dropout率\n",
    "    output_activation: 输出层的激活函数\n",
    "    seed: 随机种子\n",
    "    \"\"\"\n",
    "    \n",
    "    # 参数检查\n",
    "    # 目前只支持item_feature_columns为1的情况\n",
    "    if len(item_feature_columns)>1:\n",
    "        raise ValueError('Now MIND only support 1 item feature like item_id')\n",
    "    \n",
    "    # 获取物品特征配置\n",
    "    # 从item_feature_columns中提取物品特征的名称、词汇表大小和嵌入维度\n",
    "    # 获取item相关的配置参数\n",
    "    item_feature_column=item_feature_columns[0]\n",
    "    item_feature_name=item_feature_column.name\n",
    "    item_vocabulary_size=item_feature_column.vocabulary_size\n",
    "    item_embedding_dim=item_feature_column.embedding_dim\n",
    "    \n",
    "    behavior_feature_list=[item_feature_name]\n",
    "    \n",
    "    # 利用build_input_layers为用户和物品特征构建输入层\n",
    "    # 为用户特征创建input层\n",
    "    user_input_layer_dict=build_input_layers(user_feature_columns)\n",
    "    item_input_layer_dict=build_input_layers(item_feature_columns)\n",
    "    # 将input层转换成列表的形式作为model的输入\n",
    "    user_input_layers=list(user_input_layer_dict.values())\n",
    "    item_input_layers=list(item_input_layer_dict.values())\n",
    "    \n",
    "    # 特征筛选：筛选出用户特征中的稀疏特征、密集特征和变长稀疏特征\n",
    "    # 筛选出特征中的sparse特征和dense特征，方便单独处理\n",
    "    sparse_feature_columns=list(filter(lambda x: isinstance(x, SparseFeat), user_feature_columns)) if user_feature_columns else []\n",
    "    dense_feature_columns=list(filter(lambda x: isinstance(x, DenseFeat), user_feature_columns)) if user_feature_columns else []\n",
    "    varlen_feature_columns=list(filter(lambda x: isinstance(x, VarLenSparseFeat), user_feature_columns)) if user_feature_columns else []\n",
    "    \n",
    "    # 由于这个变长序列俩面只有历史点击文章，没有类别等，所以这里可以直接使用varlen_feature_columns\n",
    "    # deepctr这里单独把点击文章放到了history_feature_columns\n",
    "    seq_max_len=varlen_feature_columns[0].maxlen\n",
    "    \n",
    "    # 使用build_embedding_layers构建嵌入层\n",
    "    embedding_layer_dict=build_embedding_layers(user_feature_columns+item_feature_columns)\n",
    "    \n",
    "    # 使用embedding_lookup函数获取行为特征和用户特征的嵌入向量\n",
    "    # 获取当前的行为特征的embedding，这里面可能有多个类别特征，因此需要pooling\n",
    "    query_embed_list=embedding_lookup(behavior_feature_list, item_input_layer_dict, embedding_layer_dict) # 长度为1\n",
    "    # 获取行为序列(doc_id序列,hist_doc_id)对应的embedd，这里有可能有多个行为产生了行为序列，所以需要使用列表将其放在一起\n",
    "    keys_embed_list=embedding_lookup([varlen_feature_columns[0].name], user_input_layer_dict, embedding_layer_dict) # 长度为1\n",
    "    \n",
    "    # 用户离散特征的输入层和embedding层拼接\n",
    "    dnn_input_emb_list=embedding_lookup([col.name for col in sparse_feature_columns],user_input_layer_dict, embedding_layer_dict)\n",
    "    \n",
    "    # 获取dense\n",
    "    dnn_dense_input=[]\n",
    "    for fc in dense_feature_columns:\n",
    "        if fc.name!='hist_len': # 连续特征不要这个\n",
    "            dnn_dense_input.append(user_input_layer_dict[fc.name])\n",
    "            \n",
    "    # 把keys_emb_list和query_emb_listpooling操作\n",
    "    # 因为每个商品不仅有id，还可能有类别、品牌等多个embedding向量，这种需要pooling成一个\n",
    "    history_emb=PoolingLayer()(NoMask()(keys_embed_list))\n",
    "    target_emb=PoolingLayer()(NoMask()(query_embed_list))\n",
    "    \n",
    "    hist_len=user_input_layer_dict['hist_len']\n",
    "    \n",
    "    # 胶囊网络，用于学习用户的兴趣表示\n",
    "    high_capsule=CapsuleLayer(input_units=item_embedding_dim, out_units=item_embedding_dim, \n",
    "                              max_len=seq_max_len, k_max=k_max)((history_emb,hist_len))\n",
    "    \n",
    "    # 把用户的其他特征拼接到胶囊网络上\n",
    "    if len(dnn_input_emb_list)>0 or len(dnn_dense_input)>0:\n",
    "        user_other_feature=combined_dnn_input(dnn_input_emb_list, dnn_dense_input)\n",
    "        other_feature_tile=tf.keras.layers.Lambda(tile_user_otherfeat, aruments={'k_max':k_max})(user_other_feature)\n",
    "        user_deep_input=Concatenate()([NoMask()(other_feature_tile),high_capsule])\n",
    "    else:\n",
    "        user_deep_input=high_capsule  \n",
    "        \n",
    "    # 接下来经过一个DNN层，获取最终的用户表示向量\n",
    "    user_embeddings=DNN(user_dnn_hidden_units, dnn_activation, reg_dnn, dnn_dropout, dnn_use_bn, output_activation=output_activation, seed=seed, name='user_embedding')(user_deep_input)\n",
    "    \n",
    "    # 接下来，过Label-aware Layer\n",
    "    # 利用LabelAwareAttention来增强用户表示，使其更加关注用户的历史行为\n",
    "    if dynamic_k:\n",
    "        user_embedding_final=LabelAwareAttention(k_max, k_max, pow_p=p, )((user_embedding, target_emb, hist_len))\n",
    "    else:\n",
    "        user_embedding_final=LabelAwareAttention(k_max=k_max, pow_p=p, )((user_embedding, target_emb))\n",
    "        \n",
    "    item_embedding_matrix=embedding_layer_dict[item_feature_name] # 获取doc_id的embedding层\n",
    "    item_index=EmbeddingIndex(list(range(item_vocabulary_size)))(item_input_layer_dict[item_feature_name]) # 所有doc_id的索引  \n",
    "    item_embedding_weight=NoMask()(item_embedding_matrix(item_index)) # 拿到所有item的embedding\n",
    "    pooling_item_embedding_weight=PoolingLayer()([item_embedding_weight])\n",
    "    \n",
    "    # 传入整个doc_id的embedding、user_embedding，以及用户点击的doc_id，然后进行负采样计算损失操作\n",
    "    output=SampledSoftmaxLayer(num_sampled)([pooling_item_embedding_weight, user_embedding_final, item_input_layer_dict[item_feature_name]])\n",
    "    \n",
    "    # 使用Model类构建整个推荐系统模型，包括用户和物品的输入层、嵌入层、胶囊网络、DNN层和输出层\n",
    "    model=Model(inputs=user_input_layers+item_input_layers,outputs=output)\n",
    "    \n",
    "    # 等模型训练完之后，获取用户和item的embedding\n",
    "    model.__setattr__('user_input',user_input_layers)\n",
    "    model.__setattr__('user_embedding',user_embeddings)\n",
    "    model.__setattr__('item_input',item_input_layers)\n",
    "    model.__setattr__('item_embedding',get_item_embedding(pooling_item_embedding_weight, item_input_layer_dict[item_feature_name]))\n",
    "    \n",
    "    return model # 返回构建好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数式API搭建模型\n",
    "# 需要传入封装好的用户特征描述以及item特征描述\n",
    "# 建立模型\n",
    "user_feature_columns = [\n",
    "        SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),\n",
    "        VarLenSparseFeat(SparseFeat('hist_doc_ids', feature_max_idx['article_id'], embedding_dim,\n",
    "                                                        embedding_name=\"click_doc_id\"), his_seq_maxlen, 'mean', 'hist_len'),    \n",
    "        DenseFeat('hist_len', 1),\n",
    "        SparseFeat('u_city', feature_max_idx['city'], embedding_dim),\n",
    "        SparseFeat('u_age', feature_max_idx['age'], embedding_dim),\n",
    "        SparseFeat('u_gender', feature_max_idx['gender'], embedding_dim),\n",
    "    ]\n",
    "doc_feature_columns = [\n",
    "    SparseFeat('click_doc_id', feature_max_idx['article_id'], embedding_dim)\n",
    "    # 这里后面也可以把文章的类别画像特征加入\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0b/fx5b7rgn6rl61000z85h05fw0000gn/T/ipykernel_65455/4289079893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 构建模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMIND\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_feature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/0b/fx5b7rgn6rl61000z85h05fw0000gn/T/ipykernel_65455/1411853874.py\u001b[0m in \u001b[0;36mMIND\u001b[0;34m(user_feature_columns, item_feature_columns, num_sampled, k_max, p, dynamic_k, user_dnn_hidden_units, dnn_activation, dnn_use_bn, reg_dnn, reg_embedding, dnn_dropout, output_activation, seed)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# 由于这个变长序列俩面只有历史点击文章，没有类别等，所以这里可以直接使用varlen_feature_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# deepctr这里单独把点击文章放到了history_feature_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mseq_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvarlen_feature_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# 使用build_embedding_layers构建嵌入层\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = MIND(user_feature_columns, item_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把keys_emb_list和query_emb_listpooling操作\n",
    "history_emb=PoolingLayer()(NoMask()(keys_embed_list))\n",
    "target_emb=PoolingLayer()(NoMask()(query_embed_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 胶囊网络\n",
    "high_capsule=CapsuleLayer(input_units=item_embedding_dim, out_units=item_embedding_dim, max_len=seq_max_len, k_max=k_max)((history_emb, hist_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把用户的其他特征拼接到胶囊网络上\n",
    "if len(dnn_input_emb_list)>0 or len(dnn_dense_input)>0:\n",
    "    user_other_feature=combined_dnn_input(dnn_input_emb_list, dnn_dense_input)\n",
    "    other_feature_tile=tf.keras.layers.Lambda(tile_user_otherfeat, argument={'k_max':k_max})(user_other_feature)\n",
    "    user_deep_input=Concatenate()([NoMask()(other_feature_tile), high_capsule])\n",
    "else:\n",
    "    user_deep_input=high_capsule\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过一个DNN层，获取最终的用户表示向量\n",
    "user_embeddings = DNN(user_dnn_hidden_units, dnn_activation, l2_reg_dnn,\n",
    "                          dnn_dropout, dnn_use_bn, output_activation=output_activation, seed=seed,\n",
    "                          name=\"user_embedding\")(user_deep_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过LabelAwareAttention层\n",
    "# 对这两个兴趣向量与当前item的相关性加注意力权重，最后变成1个用户的最终向量\n",
    "user_embedding_final = LabelAwareAttention(k_max=k_max, pow_p=p,)((user_embeddings, target_emb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
