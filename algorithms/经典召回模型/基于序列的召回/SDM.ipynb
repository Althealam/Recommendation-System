{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"构造sdm数据集\"\"\"\n",
    "def get_data_set(click_data, seq_short_len=5, seq_prefer_len=50):\n",
    "    \"\"\"\n",
    "    :param: seq_short_len: 短期会话的长度\n",
    "    :param: seq_prefer_len: 会话的最长长度\n",
    "    \"\"\"\n",
    "    click_data.sort_values(\"expo_time\", inplace=True)\n",
    "    \n",
    "    train_set, test_set = [], []\n",
    "    for user_id, hist_click in tqdm(click_data.groupby('user_id')):\n",
    "        pos_list = hist_click['article_id'].tolist()\n",
    "        cat1_list = hist_click['cat_1'].tolist()\n",
    "        cat2_list = hist_click['cat_2'].tolist()\n",
    "        \n",
    "        # 滑动窗口切分数据\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            cat1_hist = cat1_list[:i]\n",
    "            cat2_hist = cat2_list[:i]\n",
    "            # 序列长度只够短期的\n",
    "            if i <= seq_short_len and i != len(pos_list) - 1:\n",
    "                train_set.append((\n",
    "                    # 用户id, 用户短期历史行为序列， 用户长期历史行为序列， 当前行为文章， label， \n",
    "                    user_id, hist[::-1], [0]*seq_prefer_len, pos_list[i], 1, \n",
    "                    # 用户短期历史序列长度， 用户长期历史序列长度， \n",
    "                    len(hist[::-1]), 0, \n",
    "                    # 用户短期历史序列对应类别1， 用户长期历史行为序列对应类别1\n",
    "                    cat1_hist[::-1], [0]*seq_prefer_len, \n",
    "                    # 历史短期历史序列对应类别2， 用户长期历史行为序列对应类别2 \n",
    "                    cat2_hist[::-1], [0]*seq_prefer_len\n",
    "                ))\n",
    "            # 序列长度够长期的\n",
    "            elif i != len(pos_list) - 1:\n",
    "                train_set.append((\n",
    "                    # 用户id, 用户短期历史行为序列，用户长期历史行为序列， 当前行为文章， label\n",
    "                    user_id, hist[::-1][:seq_short_len], hist[::-1][seq_short_len:], pos_list[i], 1, \n",
    "                    # 用户短期行为序列长度，用户长期行为序列长度，\n",
    "                    seq_short_len, len(hist[::-1])-seq_short_len,\n",
    "                    # 用户短期历史行为序列对应类别1， 用户长期历史行为序列对应类别1\n",
    "                    cat1_hist[::-1][:seq_short_len], cat1_hist[::-1][seq_short_len:],\n",
    "                    # 用户短期历史行为序列对应类别2， 用户长期历史行为序列对应类别2\n",
    "                    cat2_hist[::-1][:seq_short_len], cat2_hist[::-1][seq_short_len:]             \n",
    "                ))\n",
    "            # 测试集保留最长的那一条\n",
    "            elif i <= seq_short_len and i == len(pos_list) - 1:\n",
    "                test_set.append((\n",
    "                    user_id, hist[::-1], [0]*seq_prefer_len, pos_list[i], 1,\n",
    "                    len(hist[::-1]), 0, \n",
    "                    cat1_hist[::-1], [0]*seq_perfer_len, \n",
    "                    cat2_hist[::-1], [0]*seq_prefer_len\n",
    "                ))\n",
    "            else:\n",
    "                test_set.append((\n",
    "                    user_id, hist[::-1][:seq_short_len], hist[::-1][seq_short_len:], pos_list[i], 1,\n",
    "                    seq_short_len, len(hist[::-1])-seq_short_len, \n",
    "                    cat1_hist[::-1][:seq_short_len], cat1_hist[::-1][seq_short_len:],\n",
    "                    cat2_list[::-1][:seq_short_len], cat2_hist[::-1][seq_short_len:]\n",
    "                ))\n",
    "                \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "        \n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDM(user_feature_columns, item_feature_columns, history_feature_list, num_sampled=5, units=32, rnn_layers=2,\n",
    "        dropout_rate=0.2, rnn_num_res=1, num_head=4, l2_reg_embedding=1e-6, dnn_activation='tanh', seed=1024):\n",
    "    \"\"\"\n",
    "    :param rnn_num_res: rnn的残差层个数 \n",
    "    :param history_feature_list: short和long sequence field\n",
    "    \"\"\"\n",
    "    # item_feature目前只支持doc_id， 再加别的就不行了，其实这里可以改造下\n",
    "    if (len(item_feature_columns)) > 1: \n",
    "        raise ValueError(\"SDM only support 1 item feature like doc_id\")\n",
    "    \n",
    "    # 获取item_feature的一些属性\n",
    "    item_feature_column = item_feature_columns[0]\n",
    "    item_feature_name = item_feature_column.name\n",
    "    item_vocabulary_size = item_feature_column.vocabulary_size\n",
    "    \n",
    "    # 为用户特征创建Input层\n",
    "    user_input_layer_dict = build_input_layers(user_feature_columns)\n",
    "    item_input_layer_dict = build_input_layers(item_feature_columns)\n",
    "    \n",
    "    # 将Input层转化成列表的形式作为model的输入\n",
    "    user_input_layers = list(user_input_layer_dict.values())\n",
    "    item_input_layers = list(item_input_layer_dict.values())\n",
    "    \n",
    "    # 筛选出特征中的sparse特征和dense特征，方便单独处理\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), user_feature_columns)) if user_feature_columns else []\n",
    "    dense_feature_columns = list(filter(lambda x: isinstance(x, DenseFeat), user_feature_columns)) if user_feature_columns else []\n",
    "    if len(dense_feature_columns) != 0:\n",
    "        raise ValueError(\"SDM dont support dense feature\")  # 目前不支持Dense feature\n",
    "    varlen_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), user_feature_columns)) if user_feature_columns else []\n",
    "    \n",
    "    # 构建embedding字典\n",
    "    embedding_layer_dict = build_embedding_layers(user_feature_columns+item_feature_columns)\n",
    "    \n",
    "    # 拿到短期会话和长期会话列 之前的命名规则在这里起作用\n",
    "    sparse_varlen_feature_columns = []\n",
    "    prefer_history_columns = []\n",
    "    short_history_columns = []\n",
    "    \n",
    "    prefer_fc_names = list(map(lambda x: \"prefer_\" + x, history_feature_list))\n",
    "    short_fc_names = list(map(lambda x: \"short_\" + x, history_feature_list))\n",
    "    \n",
    "    for fc in varlen_feature_columns:\n",
    "        if fc.name in prefer_fc_names:\n",
    "            prefer_history_columns.append(fc)\n",
    "        elif fc.name in short_fc_names:\n",
    "            short_history_columns.append(fc)\n",
    "        else:\n",
    "            sparse_varlen_feature_columns.append(fc)\n",
    "    \n",
    "    # 获取用户的长期行为序列列表 L^u \n",
    "    # [<tf.Tensor 'emb_prefer_doc_id_2/Identity:0' shape=(None, 50, 32) dtype=float32>, <tf.Tensor 'emb_prefer_cat1_2/Identity:0' shape=(None, 50, 32) dtype=float32>, <tf.Tensor 'emb_prefer_cat2_2/Identity:0' shape=(None, 50, 32) dtype=float32>]\n",
    "    prefer_emb_list = embedding_lookup(prefer_fc_names, user_input_layer_dict, embedding_layer_dict)\n",
    "    # 获取用户的短期序列列表 S^u\n",
    "    # [<tf.Tensor 'emb_short_doc_id_2/Identity:0' shape=(None, 5, 32) dtype=float32>, <tf.Tensor 'emb_short_cat1_2/Identity:0' shape=(None, 5, 32) dtype=float32>, <tf.Tensor 'emb_short_cat2_2/Identity:0' shape=(None, 5, 32) dtype=float32>]\n",
    "    short_emb_list = embedding_lookup(short_fc_names, user_input_layer_dict, embedding_layer_dict)\n",
    "    \n",
    "    # 用户离散特征的输入层与embedding层拼接 e^u\n",
    "    user_emb_list = embedding_lookup([col.name for col in sparse_feature_columns], user_input_layer_dict, embedding_layer_dict)\n",
    "    user_emb = concat_func(user_emb_list)\n",
    "    user_emb_output = Dense(units, activation=dnn_activation, name='user_emb_output')(user_emb)  # (None, 1, 32)\n",
    "    \n",
    "    # 长期序列行为编码\n",
    "    # 过AttentionSequencePoolingLayer --> Concat --> DNN\n",
    "    prefer_sess_length = user_input_layer_dict['prefer_sess_length']\n",
    "    prefer_att_outputs = []\n",
    "    # 遍历长期行为序列\n",
    "    for i, prefer_emb in enumerate(prefer_emb_list):\n",
    "        prefer_attention_output = AttentionSequencePoolingLayer(dropout_rate=0)([user_emb_output, prefer_emb, prefer_sess_length])\n",
    "        prefer_att_outputs.append(prefer_attention_output)\n",
    "    prefer_att_concat = concat_func(prefer_att_outputs)   # (None, 1, 64) <== Concat(item_embedding，cat1_embedding,cat2_embedding)\n",
    "    prefer_output = Dense(units, activation=dnn_activation, name='prefer_output')(prefer_att_concat)\n",
    "    # print(prefer_output.shape)   # (None, 1, 32)\n",
    "    \n",
    "    # 短期行为序列编码\n",
    "    short_sess_length = user_input_layer_dict['short_sess_length']\n",
    "    short_emb_concat = concat_func(short_emb_list)   # (None, 5, 64)   这里注意下， 对于短期序列，描述item的side info信息进行了拼接\n",
    "    short_emb_input = Dense(units, activation=dnn_activation, name='short_emb_input')(short_emb_concat)  # (None, 5, 32)\n",
    "    # 过rnn 这里的return_sequence=True， 每个时间步都需要输出h\n",
    "    short_rnn_output = DynamicMultiRNN(num_units=units, return_sequence=True, num_layers=rnn_layers, \n",
    "                                       num_residual_layers=rnn_num_res,   # 这里竟然能用到残差\n",
    "                                       dropout_rate=dropout_rate)([short_emb_input, short_sess_length])\n",
    "    # print(short_rnn_output) # (None, 5, 32)\n",
    "    # 过MultiHeadAttention  # (None, 5, 32)\n",
    "    short_att_output = MultiHeadAttention(num_units=units, head_num=num_head, dropout_rate=dropout_rate)([short_rnn_output, short_sess_length]) # (None, 5, 64)\n",
    "    # user_attention # (None, 1, 32)\n",
    "    short_output = UserAttention(num_units=units, activation=dnn_activation, use_res=True, dropout_rate=dropout_rate)([user_emb_output, short_att_output, short_sess_length])\n",
    "    \n",
    "    # 门控融合\n",
    "    gated_input = concat_func([prefer_output, short_output, user_emb_output])\n",
    "    gate = Dense(units, activation='sigmoid')(gated_input)   # (None, 1, 32)\n",
    "    \n",
    "    # temp = tf.multiply(gate, short_output) + tf.multiply(1-gate, prefer_output)  感觉这俩一样？\n",
    "    gated_output = Lambda(lambda x: tf.multiply(x[0], x[1]) + tf.multiply(1-x[0], x[2]))([gate, short_output, prefer_output])  # [None, 1,32]\n",
    "    gated_output_reshape = Lambda(lambda x: tf.squeeze(x, 1))(gated_output)  # (None, 32)  这个维度必须要和docembedding层的维度一样，否则后面没法sortmax_loss\n",
    "    \n",
    "    # 接下来\n",
    "    item_embedding_matrix = embedding_layer_dict[item_feature_name]  # 获取doc_id的embedding层\n",
    "    item_index = EmbeddingIndex(list(range(item_vocabulary_size)))(item_input_layer_dict[item_feature_name]) # 所有doc_id的索引\n",
    "    item_embedding_weight = NoMask()(item_embedding_matrix(item_index))  # 拿到所有item的embedding\n",
    "    pooling_item_embedding_weight = PoolingLayer()([item_embedding_weight])  # 这里依然是当可能不止item_id，或许还有brand_id, cat_id等，需要池化\n",
    "    \n",
    "    # 这里传入的是整个doc_id的embedding， user_embedding, 以及用户点击的doc_id，然后去进行负采样计算损失操作\n",
    "    output = SampledSoftmaxLayer(num_sampled)([pooling_item_embedding_weight, gated_output_reshape, item_input_layer_dict[item_feature_name]])\n",
    "    \n",
    "    model = Model(inputs=user_input_layers+item_input_layers, outputs=output)\n",
    "    \n",
    "    # 下面是等模型训练完了之后，获取用户和item的embedding\n",
    "    model.__setattr__(\"user_input\", user_input_layers)\n",
    "    model.__setattr__(\"user_embedding\", gated_output_reshape)  # 用户embedding是取得门控融合的用户向量\n",
    "    model.__setattr__(\"item_input\", item_input_layers)\n",
    "    # item_embedding取得pooling_item_embedding_weight, 这个会发现是负采样操作训练的那个embedding矩阵\n",
    "    model.__setattr__(\"item_embedding\", get_item_embedding(pooling_item_embedding_weight, item_input_layer_dict[item_feature_name]))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数式API搭建模型\n",
    "# 建立模型\n",
    "user_feature_columns = [\n",
    "    SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
    "    SparseFeat('gender', feature_max_idx['gender'], 16),\n",
    "     SparseFeat('age', feature_max_idx['age'], 16),\n",
    "    SparseFeat('city', feature_max_idx['city'], 16),\n",
    "        \n",
    "    VarLenSparseFeat(SparseFeat('short_doc_id', feature_max_idx['article_id'], embedding_dim, embedding_name=\"doc_id\"), SEQ_LEN_short, 'mean', 'short_sess_length'),    \n",
    "    VarLenSparseFeat(SparseFeat('prefer_doc_id', feature_max_idx['article_id'], embedding_dim, embedding_name='doc_id'), SEQ_LEN_prefer, 'mean', 'prefer_sess_length'),\n",
    "    VarLenSparseFeat(SparseFeat('short_cat1', feature_max_idx['cat_1'], embedding_dim, embedding_name='cat_1'), SEQ_LEN_short, 'mean', 'short_sess_length'),\n",
    "    VarLenSparseFeat(SparseFeat('prefer_cat1', feature_max_idx['cat_1'], embedding_dim, embedding_name='cat_1'), SEQ_LEN_prefer, 'mean', 'prefer_sess_length'),\n",
    "    VarLenSparseFeat(SparseFeat('short_cat2', feature_max_idx['cat_2'], embedding_dim, embedding_name='cat_2'), SEQ_LEN_short, 'mean', 'short_sess_length'),\n",
    "    VarLenSparseFeat(SparseFeat('prefer_cat2', feature_max_idx['cat_2'], embedding_dim, embedding_name='cat_2'), SEQ_LEN_prefer, 'mean', 'prefer_sess_length'),\n",
    "    ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('doc_id', feature_max_idx['article_id'], embedding_dim)]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
