{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建物品图\n",
    "# 对用户的下单（type=2）行为序列进行session划分，其中30分钟没有产生下一个行为，划分为一个session\n",
    "# 用于处理用户数据，将用户的交互行为划分为不同的会话（session）\n",
    "# 会话通常指的是用户在一定时间内的一系列连续行为\n",
    "# 这个函数的目的是为了将用户的下单行为根据时间间隔划分为会话\n",
    "def cnt_session(data,time_cnt=30,cut_type=2):\n",
    "    # 商品属性 id 被交互时间 商品种类\n",
    "    # time_cnt：表示会话划分的时间阈值（单位为分钟）\n",
    "    # cut_type：表示用于会话划分的行为类型\n",
    "    sku_list=data['sku_id'] # 商品ID列表\n",
    "    time_list=data['action_name'] # 行为时间列表\n",
    "    type_list=data['type'] # 行为类型列表\n",
    "    session=[] # 存储最终的会话列表\n",
    "    tmp_session=[] # 存储当前会话的临时商品ID列表\n",
    "    for i, item in enumerate(sku_list):\n",
    "        # 两个商品之间如果被交互的时间大于1小时，划分成不同的session\n",
    "        if type_list[i]==cut_type or (i<len(sku_list)-1 and \\\n",
    "            (time_list[i+1]-time_list[i]).seconds/60>time_cnt) or i==len(sku_list)-1:\n",
    "            tmp_session.append(item)\n",
    "            session.append(tmp_session)\n",
    "            tmp_session=[]\n",
    "        else:\n",
    "            tmp_session.append(item)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'action_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'action_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0b/fx5b7rgn6rl61000z85h05fw0000gn/T/ipykernel_60737/298641238.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 调用函数，生成会话列表\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msession_list_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnt_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/0b/fx5b7rgn6rl61000z85h05fw0000gn/T/ipykernel_60737/1599412623.py\u001b[0m in \u001b[0;36mcnt_session\u001b[0;34m(data, time_cnt, cut_type)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 商品属性 id 被交互时间 商品种类\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msku_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sku_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtime_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtype_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'action_name'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "\n",
    "# 假设我们有一些用户行为数据，包含商品ID、行为时间和行为类型\n",
    "data = {\n",
    "    'sku_id': ['A', 'B', 'C', 'A', 'D', 'B', 'E', 'C'],\n",
    "    'action_time': [datetime(2024, 5, 1, 10, 0), datetime(2024, 5, 1, 10, 5), datetime(2024, 5, 1, 10, 10),\n",
    "                    datetime(2024, 5, 1, 10, 15), datetime(2024, 5, 1, 10, 20), datetime(2024, 5, 1, 10, 25),\n",
    "                    datetime(2024, 5, 1, 10, 30), datetime(2024, 5, 1, 10, 35)],\n",
    "    'type': [2, 2, 2, 2, 2, 2, 2, 2]  # 假设2代表下单行为\n",
    "}\n",
    "\n",
    "# 将数据转换为DataFrame（这里用字典模拟）\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 调用函数，生成会话列表\n",
    "session_list_all = cnt_session(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session_list_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0b/fx5b7rgn6rl61000z85h05fw0000gn/T/ipykernel_60737/3633673186.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnode_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 存储物品对的共现频次\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 遍历所有的session list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msession_list_all\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 遍历所有的会话列表\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# 将session共现的item存到node_pair中，用于构建item-item图\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# 将共现次数所谓边的权重，即node_pair的key为边，value为边的权重（共现次数）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'session_list_all' is not defined"
     ]
    }
   ],
   "source": [
    "# 构建图\n",
    "# 计算所有session中，相邻的物品共现频次（通过字典计算）\n",
    "# 通过入度节点、出度节点以及权重分别转化成list，通过network构建有向图\n",
    "node_pair=dict() # 存储物品对的共现频次\n",
    "# 遍历所有的session list\n",
    "for session in session_list_all: # 遍历所有的会话列表\n",
    "    # 将session共现的item存到node_pair中，用于构建item-item图\n",
    "    # 将共现次数所谓边的权重，即node_pair的key为边，value为边的权重（共现次数）\n",
    "    \n",
    "    # 在每个会话中，遍历物品列表，计算相邻物品对的共现频次\n",
    "    # 对于会话中的每个物品（除了最后一个），检查它和下一个物品组成的对(session[i-1], session[i])是否已经在node_pair字典的键中\n",
    "    if (session[i-1],session[i]) not in node_pair.keys():\n",
    "        node_pair[(session[i-1],session[i])]=1 # 如果物品对不在字典的键中，将其添加到字典中，并且权重设置为1\n",
    "    else:\n",
    "        node_pair[(session[i-1],session[i])]+=1 # 如果这个物品对已在字典中，权重加1\n",
    "\n",
    "in_node_list=list(map(lambda x: x[0],list(node_pair.keys())))\n",
    "out_node_list=list(map(lambda x: x[1],list(node_pair.keys())))\n",
    "weight_list=list(node_pair.values())\n",
    "graph_list=list([(i,o,w) for i,o,w in zip(in_node_list,out_node_list,weight_list)])\n",
    "# 使用networkx创建一个有向图\n",
    "# i是入度节点，o是出度节点，w是权重\n",
    "G=nx.DiGraph().add_weighted_edges_from(graph_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到的G是一个有向图。G包含以下内容：\n",
    "1. 节点：图中的每个节点代表一个物品，节点的标识符是物品的sku_id；\n",
    "2. 边：代表两个物品之间的共现关系。如果在一个会话中，物品 A 被用户交互后紧接着物品 B 也被用户交互，那么在图 G 中就会存在一条从 A 指向 B 的边。\n",
    "3. 权重：表示这两个物品共现的次数。权重越高，表示这两个物品在会话中相邻出现的次数越多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机游走\n",
    "# 基于构建的图进行随机游走，其中p和q是参数，用于控制采样偏向于DFS还是BFS，其实也就是node2vec\n",
    "walker=RandomWalker(G,p=args.p,q=args.q)\n",
    "print('Processes transition probs...')\n",
    "walker.preprocess_transition_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transition_probs(self):\n",
    "    \"\"\"预处理随机游走的转移概率\"\"\"\n",
    "    G=self.G\n",
    "    alias_nodes={}\n",
    "    for node in G.nodes():\n",
    "        # 获取每个节点与邻居节点边上的权重\n",
    "        unnormalized_probs=[G[node][nbr].get('weight',1.0) for nbr in G.neighbors(node)]\n",
    "        norm_const=sum(unnormalized_probs)\n",
    "        # 对每个节点的邻居权重进行归一化\n",
    "        normalized_probs=[\n",
    "            float(u_prob)/norm_const for u_prob in unnormalized_probs\n",
    "        ]\n",
    "        # 根据权重建立alias表\n",
    "        alias_nodes[node]=create_alias_table(normalized_probs)\n",
    "    alias_edges={}\n",
    "    for edge in G.edges():\n",
    "        # 获取边的alias\n",
    "        alias_edges[edge]=self.get_alias_edge(edge[0],edge[1])\n",
    "    self.alias_nodes=alias_nodes\n",
    "    self.alias_edges=alias_edges\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建好Alias后，进行带权重的随机游走\n",
    "session_reproduce=walker.simulate_walks(num_walks=args.num_walks,walk_length=args.walk_length,workers=4,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_walks(self,nodes,num_walks,walk_length,):\n",
    "    walks=[]\n",
    "    for _ in range(num_walks):\n",
    "        # 打乱所有起始节点\n",
    "        random.shuffle(nodes)\n",
    "        for v in nodes:\n",
    "            # 根据p和q选择随机游走或者带权游走\n",
    "            if self.p==1 and self.q==1:\n",
    "                walks.append(self.deepwalk_walk(walk_length=walk_length,start_node=v))\n",
    "            else:\n",
    "                walks.append(self.node2vec_walk(\n",
    "                    walk_length=walk_length, start_node=v\n",
    "                ))\n",
    "        return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载side information并构造训练正样本\n",
    "# 将所有的sku和其对应的side information进行left join，没有的特征用0补充\n",
    "# 然后对所有的特征进行labelEncoder\n",
    "sku_side_info = pd.merge(all_skus, product_data, on='sku_id', how='left').fillna(0) # 为商品加载side information\n",
    "for feat in sku_side_info.columns:\n",
    "    if feat != 'sku_id':\n",
    "        lbe = LabelEncoder()\n",
    "        # 对side information进行编码\n",
    "        sku_side_info[feat] = lbe.fit_transform(sku_side_info[feat])\n",
    "    else:\n",
    "        sku_side_info[feat] = sku_lbe.transform(sku_side_info[feat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_context_all_pairs(walks, window_size):\n",
    "    all_pairs = []\n",
    "    for k in range(len(walks)):\n",
    "        for i in range(len(walks[k])):\n",
    "            # 通过窗口的方式采取正样本，具体的是，让随机游走序列的起始item与窗口内的每个item组成正样本对\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                if i == j or j < 0 or j >= len(walks[k]):\n",
    "                    continue\n",
    "                else:\n",
    "                    all_pairs.append([walks[k][i], walks[k][j]])\n",
    "    return np.array(all_pairs, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGES模型\n",
    "def EGES(side_information_columns, items_columns, merge_type = \"weight\", share_flag=True,\n",
    "        l2_reg=0.0001, seed=1024):\n",
    "    # side_information 所对应的特征\n",
    "    feature_columns = list(set(side_information_columns))\n",
    "    # 获取输入层，查字典\n",
    "    feature_encode = FeatureEncoder(feature_columns,  linear_sparse_feature=None)\n",
    "    # 输入的值\n",
    "    feature_inputs_list = list(feature_encode.feature_input_layer_dict.values())\n",
    "    # item id  获取输入层的值\n",
    "    items_Map = FeatureMap(items_columns)\n",
    "    items_inputs_list = list(items_Map.feature_input_layer_dict.values())\n",
    "\n",
    "    # 正样本的id，在softmax中需要传入正样本的id\n",
    "    label_columns = [DenseFeat('label_id', 1)]\n",
    "    label_Map = FeatureMap(label_columns)\n",
    "    label_inputs_list = list(label_Map.feature_input_layer_dict.values())\n",
    "\n",
    "    # 通过输入的值查side_information的embedding，返回所有side_information的embedding的list\n",
    "    side_embedding_list = process_feature(side_information_columns, feature_encode)\n",
    "    # 拼接  N x num_feature X Dim\n",
    "    side_embeddings = Concatenate(axis=1)(side_embedding_list)\n",
    "\n",
    "    # items_inputs_list[0] 为了查找每个item 用于计算权重的 aplha 向量\n",
    "    eges_inputs = [side_embeddings, items_inputs_list[0]]\n",
    "\n",
    "    merge_emb = EGESLayer(items_columns[0].vocabulary_size, merge_type=merge_type, \n",
    "                l2_reg=l2_reg, seed=seed)(eges_inputs)  # B * emb_dim\n",
    "    \n",
    "    label_idx = label_Map.feature_input_layer_dict[label_columns[0].name]\n",
    "    softmaxloss_inputs = [merge_emb,label_idx]\n",
    "    \n",
    "    item_vocabulary_size = items_columns[0].vocabulary_size\n",
    "\n",
    "    all_items_idx = EmbeddingIndex(list(range(item_vocabulary_size)))\n",
    "    all_items_embeddings = feature_encode.embedding_layers_dict[side_information_columns[0].name](all_items_idx)\n",
    "\n",
    "    if share_flag:\n",
    "        softmaxloss_inputs.append(all_items_embeddings)\n",
    "    \n",
    "    output = SampledSoftmaxLayer(num_items=item_vocabulary_size, share_flage=share_flag,\n",
    "              emb_dim=side_information_columns[0].embedding_dim,num_sampled=10)(softmaxloss_inputs)\n",
    "\n",
    "    model = Model(feature_inputs_list + items_inputs_list + label_inputs_list, output)\n",
    "    \n",
    "    model.__setattr__(\"feature_inputs_list\", feature_inputs_list)\n",
    "    model.__setattr__(\"label_inputs_list\", label_inputs_list)\n",
    "    model.__setattr__(\"merge_embedding\", merge_emb)\n",
    "    model.__setattr__(\"item_embedding\", get_item_embedding(all_items_embeddings,                                                          items_Map.feature_input_layer_dict[items_columns[0].name]))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGESLayer为聚合每个item的多个side information的方法\n",
    "# 其中merge_typ2可以选择average_pooling或者weight_pooling\n",
    "class EGESLayer(Layer):\n",
    "    def __init__(self,item_nums, merge_type=\"weight\",l2_reg=0.001,seed=1024, **kwargs):\n",
    "        super(EGESLayer, self).__init__(**kwargs)\n",
    "        self.item_nums = item_nums \n",
    "        self.merge_type = merge_type   #聚合方式\n",
    "        self.l2_reg = l2_reg\n",
    "        self.seed = seed\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) < 2:\n",
    "            raise ValueError('`EGESLayer` layer should be called \\\n",
    "                on a list of at least 2 inputs')\n",
    "        self.feat_nums = input_shape[0][1]\n",
    "        \n",
    "        if self.merge_type == \"weight\":\n",
    "            self.alpha_embeddings = self.add_weight(\n",
    "                                name='alpha_attention',\n",
    "                                shape=(self.item_nums, self.feat_nums),\n",
    "                                dtype=tf.float32, \n",
    "                                initializer=tf.keras.initializers.RandomUniform(minval=-1, maxval=1,                                               seed=self.seed),\n",
    "                                regularizer=l2(self.l2_reg))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.merge_type == \"weight\": \n",
    "            stack_embedding = inputs[0]  # (B * num_feate * embedding_size)\n",
    "            item_input = inputs[1]       # (B * 1)  \n",
    "            alpha_embedding = tf.nn.embedding_lookup(self.alpha_embeddings, item_input) #(B * 1 * num_feate)\n",
    "            alpha_emb = tf.exp(alpha_embedding) \n",
    "            alpha_i_sum = tf.reduce_sum(alpha_emb, axis=-1) \n",
    "            merge_embedding = tf.squeeze(tf.matmul(alpha_emb, stack_embedding),axis=1) / alpha_i_sum\n",
    "        else:\n",
    "            stack_embedding = inputs[0]  # (B * num_feate * embedding_size)\n",
    "            merge_embedding = tf.squeeze(tf.reduce_mean(alpha_emb, axis=1),axis=1) # (B * embedding_size)\n",
    "        \n",
    "        return merge_embedding\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"merge_type\": self.merge_type, \"seed\": self.seed}\n",
    "        base_config = super(EGESLayer, self).get_config()\n",
    "        base_config.update(config)\n",
    "        return base_config\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
